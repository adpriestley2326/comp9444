{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [],
            "source": [
                "%matplotlib inline"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Convolutional Neural Network\n",
                "=====================\n",
                "\n",
                "In this exercise, you will practice how to impelement a convolutional neural network (CNN) for image classification with PyTorch. Specifically, you need to implement one of the most famous CNN - the LeNet, and apply it on a handwritten digits dataset - MNIST. After finishing building the network, you also need to run the training algorithm and compare the performance of LeNet and a multi-layer perceptron (We've already implemented for you). You can also do some hyperparameter tuning or model modification to check how it will affect the classification performance.\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "Training an image classifier\n",
                "----------------------------\n",
                "\n",
                "Normally, the algorithm for training a image classifier includes the  following steps:\n",
                "\n",
                "1. Load and normalize the training and test datasets using ``torchvision``\n",
                "2. Define a neural network model\n",
                "3. Define a loss function and optimizer\n",
                "4. Train the network on the training data\n",
                "5. Validate the network on the validation data\n",
                "6. Test the network on the test data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torchvision\n",
                "import torchvision.transforms as transforms"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Hyperparameters\n",
                "\n",
                "After you finish building the neural network model, you can try different values of hyperparameters and check how it will affect the performance of your model, e.g., increase/decrease batch size and learning_rate, or increase the width of the convolutional layers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: try different values of hyperparameters and check how it will affect the classification performance.\n",
                "\n",
                "batch_size=128\n",
                "learning_rate=0.0001"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "NVIDIA GeForce GTX 1050\n",
                        "27262976\n"
                    ]
                }
            ],
            "source": [
                "cuda = torch.cuda.is_available()\n",
                "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
                "print(torch.cuda.memory_reserved())"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Torchvision and datasets\n",
                "----------------\n",
                "\n",
                "PyTorch has a package called\n",
                "``torchvision``, which includes data loaders for common datasets such as\n",
                "Imagenet, CIFAR10, MNIST, etc. and data transformers for images, viz.,\n",
                "``torchvision.datasets`` and ``torch.utils.data.DataLoader``.\n",
                "\n",
                "This provides a huge convenience and avoids writing boilerplate code. For this exercise, we will use the MNIST dataset which is a large database of handwritten digits.\n",
                "\n",
                "The output of torchvision datasets are PILImage images of range [0, 1].\n",
                "We transform them to Tensors of normalized range [-1, 1]."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We normalize the data by its mean and variance.\n",
                "transform=transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize((0.1307,), (0.3081,))\n",
                "    ])\n",
                "\n",
                "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
                "                                download=True, transform=transform)\n",
                "\n",
                "\n",
                "# training validation split \n",
                "train_set, val_set = torch.utils.data.random_split(trainset, [50000, 10000])\n",
                "\n",
                "\n",
                "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
                "                                          shuffle=True, num_workers=0)\n",
                "\n",
                "valloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
                "                                          shuffle=False, num_workers=0)\n",
                "\n",
                "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
                "                                download=True, transform=transform)\n",
                "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
                "                                         shuffle=False, num_workers=0)\n",
                "\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div class=\"alert alert-info\"><h4>Note</h4><p>If running on Windows and you get a BrokenPipeError, try setting\n",
                "    the num_worker of torch.utils.data.DataLoader() to 0.</p></div>"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Build the LeNet\n",
                "----------------\n",
                "Build the network according to the instruction. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "\n",
                "# TODO: Implement the LeNet according to the description.\n",
                "class LeNet(nn.Module):\n",
                "\n",
                "    def __init__(self):\n",
                "        super(LeNet, self).__init__()\n",
                "        # Here is an example of the convolutional layer where \n",
                "        # input channel=1, output channel=6, kernel size=5, padding=2\n",
                "        # for this layer (only) we set padding=2 because LeNet is\n",
                "        # expecting an image of size 32x32 instead of 28x28\n",
                "        # implement other layers by yourself.\n",
                "        self.conv1 = nn.Conv2d(1, 6, 5, padding=2)\n",
                "        #self.pool2 = nn.MaxPool2d(2)\n",
                "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
                "        #self.pool4 = nn.MaxPool2d(2)\n",
                "        self.fc1 = nn.Linear(400, 120)\n",
                "        self.fc2 = nn.Linear(120, 84)\n",
                "        self.fc3 = nn.Linear(84, 10)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = F.relu(self.conv1(x))\n",
                "        x = F.max_pool2d(x, 2)\n",
                "        x = F.relu(self.conv2(x))\n",
                "        x = F.max_pool2d(x, 2)\n",
                "        x = x.view(x.size(0), -1)\n",
                "        x = F.relu(self.fc1(x))\n",
                "        x = F.relu(self.fc2(x))\n",
                "        x = self.fc3(x)\n",
                "        return x\n",
                "\n",
                "# We've implemented a multi-layer perceptron model so that you can try to run the training algorithm\n",
                "# and compare it with LeNet in terms of the classification performance.\n",
                "class MLP(nn.Module):\n",
                "\n",
                "    def __init__(self):\n",
                "        super(MLP, self).__init__()\n",
                "        self.input = nn.Linear(28 * 28, 512)\n",
                "        self.hidden = nn.Linear(512, 256)\n",
                "        self.output = nn.Linear(256, 10)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = x.view(-1, 28 * 28)\n",
                "        x = torch.sigmoid(self.input(x))\n",
                "        x = torch.sigmoid(self.hidden(x))\n",
                "        x = self.output(x)\n",
                "        return x\n",
                "\n",
                "net = MLP()\n",
                "\n",
                "# Uncomment this line after you implement it\n",
                "net = LeNet()\n",
                "if torch.cuda.is_available():\n",
                "    net = net.cuda()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Loss Function and Optimizer\n",
                "----------------\n",
                "Let's use a Classification Cross-Entropy loss and SGD with momentum.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.optim as optim\n",
                "\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Training the network\n",
                "----------------\n",
                "\n",
                "This is when things start to get interesting.\n",
                "We simply have to loop over our data iterator, and feed the inputs to the\n",
                "network and optimize. After each epoch, we print the statistics.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 0\n",
                        "Training Loss: 0.575615, Training Acc: 0.106440, Validation Acc: 0.137700, Test Acc: 0.135900\n",
                        "Epoch 1\n",
                        "Training Loss: 0.573391, Training Acc: 0.175880, Validation Acc: 0.243100, Test Acc: 0.236300\n",
                        "Epoch 2\n",
                        "Training Loss: 0.570766, Training Acc: 0.287400, Validation Acc: 0.331400, Test Acc: 0.327800\n",
                        "Epoch 3\n",
                        "Training Loss: 0.567114, Training Acc: 0.345400, Validation Acc: 0.371000, Test Acc: 0.368100\n",
                        "Epoch 4\n",
                        "Training Loss: 0.561497, Training Acc: 0.379160, Validation Acc: 0.408600, Test Acc: 0.410100\n",
                        "Epoch 5\n",
                        "Training Loss: 0.552038, Training Acc: 0.416960, Validation Acc: 0.440400, Test Acc: 0.442300\n",
                        "Epoch 6\n",
                        "Training Loss: 0.533422, Training Acc: 0.447740, Validation Acc: 0.488900, Test Acc: 0.483800\n",
                        "Epoch 7\n",
                        "Training Loss: 0.489433, Training Acc: 0.517420, Validation Acc: 0.584700, Test Acc: 0.584900\n",
                        "Epoch 8\n",
                        "Training Loss: 0.393238, Training Acc: 0.635440, Validation Acc: 0.687400, Test Acc: 0.695900\n",
                        "Epoch 9\n",
                        "Training Loss: 0.272466, Training Acc: 0.726180, Validation Acc: 0.761600, Test Acc: 0.769800\n",
                        "Finished Training\n"
                    ]
                }
            ],
            "source": [
                "for epoch in range(10):  # loop over the dataset multiple times\n",
                "    \n",
                "    train_loss = 0.0\n",
                "    train_acc = 0.0\n",
                "    val_loss = 0.0\n",
                "    val_acc = 0.0\n",
                "    test_loss = 0.0\n",
                "    test_acc = 0.0\n",
                "    \n",
                "    for i, data in enumerate(trainloader, 0):\n",
                "        # get the inputs; data is a list of [inputs, labels]\n",
                "        inputs, labels = data\n",
                "        inputs, labels = inputs.cuda(), labels.cuda()\n",
                "\n",
                "        # zero the parameter gradients\n",
                "        optimizer.zero_grad()\n",
                "\n",
                "        \n",
                "        \n",
                "        # forward + backward + optimize\n",
                "        outputs = net(inputs)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "        \n",
                "        \n",
                "        \n",
                "        # statistics\n",
                "        train_loss += loss.item()\n",
                "        pred = torch.max(outputs, 1)[1]\n",
                "        train_correct = (pred == labels).sum()\n",
                "        train_acc += train_correct.item()\n",
                "\n",
                "        \n",
                "    # To get the best learned model, we need to do some statisticcs.\n",
                "    # After training, we pick the model with best validation accuracy.\n",
                "    with torch.no_grad():\n",
                "        net.eval()\n",
                "\n",
                "        for inputs, labels in valloader:\n",
                "            inputs, labels = inputs.cuda(), labels.cuda()\n",
                "\n",
                "            predicts = net(inputs)\n",
                "\n",
                "            loss = criterion(predicts, labels)\n",
                "            val_loss += loss.item()\n",
                "            pred = torch.max(predicts, 1)[1]\n",
                "            val_correct = (pred == labels).sum()\n",
                "            val_acc += val_correct.item()\n",
                "\n",
                "        for inputs, labels in testloader:\n",
                "            inputs, labels = inputs.cuda(), labels.cuda()\n",
                "\n",
                "            predicts = net(inputs)\n",
                "            pred = torch.max(predicts, 1)[1]\n",
                "            test_correct = (pred == labels).sum()\n",
                "            test_acc += test_correct.item()\n",
                "\n",
                "        net.train()\n",
                "    print(\"Epoch %d\" % epoch )\n",
                "\n",
                "    print('Training Loss: {:.6f}, Training Acc: {:.6f}, Validation Acc: {:.6f}, Test Acc: {:.6f}'.format(train_loss / (len(train_set))*32,train_acc / (len(train_set)), val_acc / (len(val_set)),test_acc / (len(testset))))        \n",
                "\n",
                "print('Finished Training')"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "92643ed630b36500ed960c4faa1bf017a228a3cd951c802488bf8c80581b8124"
        },
        "kernelspec": {
            "display_name": "Python 3.7.3 64-bit ('comp9444': conda)",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
